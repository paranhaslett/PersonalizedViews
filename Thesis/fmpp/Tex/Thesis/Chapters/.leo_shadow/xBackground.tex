%@+leo-ver=4-thin
%@+node:paran.20140515173914.6982:@shadow Background.tex
%@@color
%@@language latex
\chapter{Background}

\section{Version Control Systems}
%@<<VersionControlSystems>>
%@+node:paran.20140528183434.1974:<<VersionControlSystems>>
\subsection{What is version control and why should we use it}
%@<<Reasons for using version control>>
%@+node:paran.20140530135904.1945:<<Reasons for using version control>>
Version control systems are a way of managing different revisions or versions of plain text documents. Most commonly a version control system is used to maintain source code written for a plain text programming language.  There are a number of reasons why we might want to use a version control system.

%@<<backups>>
%@+node:paran.20140530135904.1948:<<backups>>
When a version control system is used by a single person to manage their documents the revisions could simply be backups.

Before version control the only way to keep a backup of a file was to copy the code and save it into a different file on the file system.
In \"Pro Git\" Chacon observes that if you have no source control the best that you can do is to manually back up items on the file system under folders with a specific date so that you can go back and visit a previous revision \cite{Chacon}.  A problem that source code revision addresses is that when a backup is made many files are duplicated verbatim.  The needlessly duplicated files waste file resources.

 A previous revision can always be revised at a later date and changed. If there is something significant about a particular revision it can be labelled with a tag. A tag assigns a name to all the files in the revision you are interested in so that you can more easily revisit the code at a certain point.  This is helpful if a software package has a number of released versions.  If you need to go back and revisit a particular release it becomes a lot easier if you have tagged the code at that point with the release name or identification.
%@nonl
%@-node:paran.20140530135904.1948:<<backups>>
%@nl

%@<<branching>>
%@+node:paran.20140530135904.1950:<<branching>>
It is also possible to maintain two sets of documents for different purposes. This is useful if you have a project that has two or more potential uses. It is also valuable if there are some experimental feature which you want to explore but want to maintain the original. A version control system can keep these multiple interests separate by putting them on different branches.  It is still possible to easily switch between branches depending on which project you want to make changes to.  A good use of this feature is if you have a software project that you have written on behalf two different companies but each of them would like their own unique customisations on top of the base product.  By making two copies of the base product and having a record of when it was divided the branches can later be recombined to include some or all of the features that have been introduced. 

%@+at 
%@nonl
% add a diagram to explain branching
%@-at
%@@c
%@nonl
%@-node:paran.20140530135904.1950:<<branching>>
%@nl

%@<<associating metadata>>
%@+node:paran.20140530135904.1951:<<associating metadata>>
Another useful feature of version control is the ability to record meta-information beside a change or a set of changes to a document.  The reason this is useful is that you can specify what the change was for.  Instead of updating individual documents you could specify that changes over multiple documents were done for a single reason.  For instance if you had a number of documents that had an address written in them and that address changes.  Once the change has been made is possible in most version control systems to write a message when the documents are checked in.  In some version control systems this message is required to check-in any set of documents.

The reason this is useful is at a later date if queries are made about what a certain change to a document was for.  Since there is a message beside all the documents about the reason for a particular change it becomes easier to figure out the reason for the individual change we are interested in. If following the example above we were to examine a document and wonder why the address changed we could examine the check in with that change and see the message that the person changing it wrote.

When used on source code in tandem with an issue tracking system the message can contain the identification number for the bug being fixed or feature being added.  This means that anybody who is examining the revision to see the reasoning for the change has access to a lot more information via the issue tracking system.

%@+at
% show screen-shot of Bugzilla
% 
% show the commands for adding a comment to git with the Bugzilla number
% 
% show how we can get the Bugzilla number back
%@-at
%@@c
%@nonl
%@-node:paran.20140530135904.1951:<<associating metadata>>
%@nl

%@<<collaboration>>
%@+node:paran.20140530135904.1949:<<collaboration>>
The real power of a version control system is its ability to manage documents that are being changed by multiple people.  In a multiple person systems it is possible to have individual revisions that contain each persons changes. The version control system then manages the way these changes are combined into a composite product. This allows multiple people to work on the same document. In some circumstances it allows them to work on the document at the same time.

This is the main difference between a version control system and a content management system.  It might be possible for a content management system to hold previous revisions but a version control systems can manage how those different versions or revisions can be combined.  A version control system allows people with different ideas to collaborate on a single document.

In order to manage to the different version or revisions version control systems work better on plain text documents. One of the main uses for a version control system is to maintain source code written in a plain text programming language.
%@nonl
%@-node:paran.20140530135904.1949:<<collaboration>>
%@nl


%@-node:paran.20140530135904.1945:<<Reasons for using version control>>
%@nl

\subsection{Dealing with conflicts}
%@<<LockvsMerge>>
%@+node:paran.20140529075353.1935:<<LockvsMerge>>
When two people are working within the same environment there is a need to interact with each other.
There is competition for the resources for each of them to successfully do their work.
It is similar in a computer system when two people are dealing with the same set of documents and files.
There is always the risk that they will attempt to change the same document at the same time.
This can cause a conflict about how the document should be changed.
There are a few ways of dealing with these conflicts.

\subsubsection{Locking}
%@<<locking>>
%@+node:paran.20140530135904.1952:<<locking>>
We could pre-emptively require that the document is only able to be used by one person at a time and that the other person has to wait. The advantage of this is that the document is always in a consistent state. The disadvantage of this is if one person retains the document for extended periods of time it cannot be changed by anybody else. Furthermore the resulting document may be barely recognisable as the original if extensive work is done on it. If the two parties are changing distinctly different parts of the document or change the document in the same way this restriction is unnecessary.

This is how one of the original versioning systems, RCS ensured that the document stayed consistent. Tichy has explained why he considers locking in a version control system to be a good idea in the design for RCS\cite{Tichy1982}

%@-node:paran.20140530135904.1952:<<locking>>
%@nl
\subsubsection{Smaller structured units}
%@<<reduction>>
%@+node:paran.20140529174111.1936:<<reduction>>
Another way to reduce the conflicts is to split it into smaller units.  The advantage of this is that if you are using locking as above you could avoid some unnecessary restrictions. If we go back to the illustration of two people working on the same set of documents.  If instead of one person having sole possession of a document at a time that person only has possession of the page or pages they are changing. As those pages are smaller than the whole document they are likely to retain them for shorter periods.
%@nonl
%@-node:paran.20140529174111.1936:<<reduction>>
%@nl
\subsubsection{Merging two documents}
%@<<Merging>>
%@+node:paran.20140528183434.2055:<<Merging>>
Finally we could allow both parties to change the document and try to figure out what the problems are afterwards.  This resolution of anything that remains a conflict is known as a merge.

If not regularly merged is possible for the source code to diverge greatly and it becomes harder and harder to reconcile.
 According to Bertino it is possible to keep a smaller more easily deployed repository by evaluating what is necessary and what is unnecessary \cite{Bertino2012}. Although Bertino refers to unnecessary files this premise may also be applicable for the smaller blocks of code we are interested in. This suggests that maintaining a record about what is relevant and what is irrelevant may have some benefit. Version control still can have problems with merge conflicts. These issues have a far greater chance of occurring if there is a dramatic change such as refactoring
 
\subsubsection{Manual Merging}
%@<<Manual Merging>>
%@+node:paran.20140528183434.2056:<<Manual Merging>>


%@+at
% Manual merge Example
% 
% if you have two document you want to merge but no record of a starting point 
% for them both you will need to manually merge any differences as the 
% computer has no record about what the original document looked like.  
% features that they have in common will not need to be affected however a 
% decision needs to be made about any differences.
% 
% insert diagram
%@-at
%@@c
%@nonl
%@-node:paran.20140528183434.2056:<<Manual Merging>>
%@nl

\subsubsection{Automatic Merging}
%@<<Automatic Merging>>
%@+node:paran.20140528183434.2057:<<Automatic Merging>>
If there is a three way merge it is possible for the computer to calculate the merge independently provided that there are no merge conflicts.
To do 3 way merging requires 3 different revisions are required, the changes you have made, the changes made by others, and the revision that is common to them both.


%@+at
% Diagram showing that how merging could be done automatically
% if there is a change in your code but no change in others code automatic
% if there is a change in others code but not yours
% 
%@-at
%@@c
%@nonl
%@-node:paran.20140528183434.2057:<<Automatic Merging>>
%@nl
%@-node:paran.20140528183434.2055:<<Merging>>
%@nl
%@-node:paran.20140529075353.1935:<<LockvsMerge>>
%@nl

\subsection{Architecture}
%@<<architecture>>
%@+node:paran.20140530135904.1946:<<architecture>>
\subsubsection{Centralised version control}
%@<<Centralised Version control>>
%@+node:paran.20140528183434.2058:<<Centralised Version control>>
Still had the model of a central repository everyone needed to consistent
similar to RCS in that you could still check the file out but it was not as necessary to lock the file
it was possible to look at the differences between the files and then automatically merge when it was possible

centralised systems have better support for locking  

examples of centralised systems

RCS, CVS, SVN, Clearcase

%@+at
% difference between centalised version controls
%@-at
%@@c
%@nonl
%@-node:paran.20140528183434.2058:<<Centralised Version control>>
%@nl

\subsubsection{Distributed version control}
%@<<Distributed Version control>>
%@+node:paran.20140528183434.2059:<<Distributed Version control>>
The need to be connected to a central system solved a lot of issues but often had a large overhead.  

%@+at 
%@nonl
% mobile access
%@-at
%@@c 

The benefits and costs of distributed systems have long been discussed

examples of distributed version control
GIT, Mercurial, Bazaar

%@+at 
% difference between git and mecurial reference
%@-at
%@@c

\subsubsection{Using git}
%@<<Git>>
%@+node:paran.20140528183434.2060:<<Git>>
Git is a repository which is used mostly for software development.

This is done by combining all the changes to a document in a process called merging. 
In order to merge, any change an editor makes needs to be recorded and compared against the changes made by other editors.
If it is possible for those changes to co-exist then the changes will be made.
An example of changes that are considered to be able to co-exist is if all editors change a different part of the document.
If it is not possible for those changes to co-exist then there is a \"merge conflict\".
An example of a merge conflict is if any two changes on the document overlap with different values.
Before any merging can be done all of the changes need to be determined

In GIT there are a number of changes recorded for a file differences,  A file could be added, deleted, moved, copied, or modified.  The same is not true of changes within the file with only insert, delete, and modify being available.
%@nonl
%@-node:paran.20140528183434.2060:<<Git>>
%@nl
%@nonl
%@-node:paran.20140528183434.2059:<<Distributed Version control>>
%@nl
%@nonl
%@-node:paran.20140530135904.1946:<<architecture>>
%@nl

%@-node:paran.20140528183434.1974:<<VersionControlSystems>>
%@nl

\section{Collaborating using GitHub and BitBucket}
%@<<GitHub>>
%@+node:paran.20140603140023.2025:<<GitHub>>
Whist is is possible for a measure of collaboration just by using git on it own it requires that you have some method of obtaining the seperate branches on one machine before they can be merged.  One way of doing this within a company is to set up a git server.  This might suitable for projects that are closed source and have a select group of people who work on the source code.  For larger projects that have has programmers in different parts of the world a publicly accessible git server that is on the web may be a better solution.  

%@+at
% the reason i achieve included this here is because the amount of interaction 
% in git means that merge needs to be better
% add a screen shot to show the number of forks an users of a large project
%@-at
%@@c
%@nonl
%@-node:paran.20140603140023.2025:<<GitHub>>
%@nl

\section{Longest Common Subsequence}
%@<<LCS>>
%@+node:paran.20140528183434.2009:<<LCS>>
There are a number of issues in computer science that can be resolved by a longest common subsequence algorithm.
It is very good at finding the differences between two different sets of ordered information.

%@+at
% look into other uses for LCS
%@-at
%@@c

\subsection{Example}
%@<<Example of Longest common subsequence>>
%@+node:paran.20140528183434.2010:<<Example of Longest common subsequence>>
One method of discovering what has changed is to find the longest common subsequence (LCS).
A simplified example of finding the longest common subsequence is:

Imagine we have have two similar sentences that we want to compare with each other.  
We would like to know what is the same and what is different.
A longest common subsequence for the sentences would contain a list of all the characters that are the same and in the same order
So for the following sentences

\begin{verbatim}

"The quick brown fox jumps over the lazy dog"

"The rapid brown fox vaults the lazy dog"

\end{verbatim}
A longest common subsequence would be
\begin{verbatim}
"The ","i"," brown fox ","v"," the lazy dog"
\end{verbatim}
The letters that are missing from the LCS differ between the sentences.
It is possible for a comparison to have multiple LCSs.
%@nonl
%@-node:paran.20140528183434.2010:<<Example of Longest common subsequence>>
%@nl

\subsection{Difference strategies}
%@<<Git difference strategies>>
%@+node:paran.20140528183434.2011:<<Git difference strategies>>
There are a number of ways that the longest common subsequence is calculated. The algorithms used in JGit for example are the the Myers, Patience and Histogram algorithms.

%@<<Myers>>
%@+node:paran.20140528183434.2012:<<Myers>>
\subsection{Myers}
The Myers algorithm was discovered by Eugene Myers who claimed that finding the minimal differences between any two documents was the equivalent to finding the shortest or longest path in a graph \cite{Myers1986}.
%@-node:paran.20140528183434.2012:<<Myers>>
%@nl

%@<<Patience>>
%@+node:paran.20140528183434.2013:<<Patience>>
\subsection{Patience}
The patience algorithm instead of figuring out the longest common subsequence directly uses the longest increasing subsequence.
when this is used with line numbers form source code the longest common subsequence can be established.

bran Cohen

because of the way a patience 

Before the act
%@+at
% from the patience game
% need references
%@-at
%@@c 
%@-node:paran.20140528183434.2013:<<Patience>>
%@nl

%@<<Histogram>>
%@+node:paran.20140528183434.2014:<<Histogram>>
\subsection{Histogram}
A Histogram difference strategy is very similar to a patience algorithm
Instead of looking at just the unique lines between any two subsets however it can examine lines that there are multiple copies of 
%@nonl
%@-node:paran.20140528183434.2014:<<Histogram>>
%@nl
%@-node:paran.20140528183434.2011:<<Git difference strategies>>
%@nl

\subsection{The problem with LCS}
%@<<The problem with LCS>>
%@+node:paran.20140528183434.2015:<<The problem with LCS>>
There is still a problem with longest common subsequence. It does not notice changes of order in a document.  For example if we were to take the following two sentences:

%@+at 
% change this to diagrams
%@-at
%@@c

\begin{verbatim}

"The quick brown fox jumps over the lazy dog"

"The lazy brown dog jumps over the quick fox"

\end{verbatim}

The longest common subsequence of this would be

\begin{verbatim}
"The \"," brown ","o"," jumps over the ","o"
\end{verbatim}

Without further analysing the changes it is possible to conclude that instead of swapping certain words that:

\begin{verbatim}
"quick" transforms into "lazy"
"f" transforms into "d"
\"x\" transforms into \"g\"
\"lazy d\" transforms into \"quick f\"
\"g\" transforms into \"x\"
\end{verbatim}

What this thesis aims to do is to more accurately portray these changes.
In order to do this we require some information about the structure of the document.
For the above example if the computer was aware that the sentence was structured into words rather than characters the result would have been slightly different.

\begin{verbatim}
\"The \",?,\" brown \",?,\"jumps \", \"over \", \"the \",?,?
\end{verbatim}

in this situation in becomes easier to recognise that words have been swapped by comparing each of the changes with each other.  

\begin{verbatim}
\"quick \" transforms into \"lazy \" matches \"lazy\" transforms into \"quick\"
\"fox \" transforms into \"dog\" matches \"dog\" transforms into \"fox\" 
\end{verbatim}

The English language is also far too complex to notice anything that is more basic than a word for word swap.
There are words and sentences that have similar meanings but are spelled and structured differently.
%@nonl
%@-node:paran.20140528183434.2015:<<The problem with LCS>>
%@nl

\subsection{How LCS is used in differencing tools}
%@<<How diffs use LCS>>
%@+node:paran.20140528183434.2016:<<How diffs use LCS>>
The above is a simplified illustration of how general LCS works.
How most difference tools use LCS is quite different.
Most difference tools rather than comparing on a character by character difference compares line of text with each other.
Often to speed up the differencing process each line is assigned a hash code depending on its contents. 
This means that the differencing tool can work much faster as it does not need to compare each character in the line but can compare hash codes instead.
In the source code for many programming languages the white space is not relevant so many diff tools have the option of ignoring the white-space and only comparing the code.
This has an impact on the hash codes for each line as the hash code needs to be generated just from the text rather than the white spaces in the code.
%@+at
% Remember to say something white space
% and regular expression differences
% 
%@-at
%@@c
%@nonl
%@-node:paran.20140528183434.2016:<<How diffs use LCS>>
%@nl
 
%@-node:paran.20140528183434.2009:<<LCS>>
%@nl
\section{Refactoring}
%@<<Refactoring>>
%@+node:paran.20140530135904.1958:<<Refactoring>>

A common concern with coding is the need to periodically refactor the code. Refactoring does not involve changing any of features the source code or change how the compiled program functions. Refactoring simply reorganises the source code so that it is easier to read and add changes. According to Fowler et al. the main time for refactoring is when new functionality is added \cite{Fowler1999}. Similarly according to Kerievsky some of the motivations for refactoring include adding more code and understanding existing code \cite{Kerievsky2004}. As adding more functionality is one of the motivations for refactoring let us consider what happens in a multi-developer environment. Two developers could have different views on what is considered an appropriate refactoring. This is especially true if they need to add different functionality from each other. 

A simple example is illustrated as follows:

\begin{lstlisting}
public TempConv() {
  %@  <<tempConv>>
  %@+node:paran.20140530135904.1959:<<tempConv>>
  Scanner keyboard = new Scanner(System.in);
  System.out.println("Enter the temperature in Celsius");
  int celsius = keyboard.nextInt();
  System.out.println("Degrees Fahrenheit is approx " 
    + (celsius * 2 + 30) );
  keyboard.close();
  %@nonl
  %@-node:paran.20140530135904.1959:<<tempConv>>
  %@nl
}
\end{lstlisting}

Refactoring this code depends on what functionality you need to add. One developer may recognize that conversion from Celsius may be used several times throughout the code and so extract the calculations as a separate method as follows:

\begin{lstlisting}
public TempConv() {
  %@  <<tempConv2>>
  %@+node:paran.20140530135904.1960:<<tempConv2>>
  Scanner keyboard = new Scanner(System.in);
  System.out.println("Enter the temperature in Celsius");
  int celsius = keyboard.nextInt();
  System.out.println("Degrees Fahrenheit is approx " 
    + celsiusToFahrenheit(celsius));
  keyboard.close();
  %@nonl
  %@-node:paran.20140530135904.1960:<<tempConv2>>
  %@nl
}

public int celsiusToFahrenheit(int celsius){
  %@  <<celsiusToFahrenheit>>
  %@+node:paran.20140530135904.1961:<<celsiusToFahrenheit>>
  return celsius * 2 + 30;
  %@nonl
  %@-node:paran.20140530135904.1961:<<celsiusToFahrenheit>>
  %@nl
}
\end{lstlisting}

This change, in spite of producing the same output as the first, provides a number of advantages. Firstly if other programs need to convert from Celsius to Fahrenheit the new method can easily be reused. Secondly since the calculation is a crude estimation it becomes a lot clearer where the code needs to be changed to improve the formula. The ability to add a method that clearly indicates that the calculation is from Celsius to Fahrenheit helps with the readability of the code. There are also disadvantages to doing this refactoring however. If we do not care about conversion between Celsius and Fahrenheit the refactoring simply adds to the amount of code we need to wade through before understanding what the code does. An alternate way of refactoring is as follows:

\begin{lstlisting}
public TempConv(){
  %@  <<tempConv3>>
  %@+node:paran.20140530135904.1962:<<tempConv3>>
  Scanner keyboard = new Scanner(System.in);
  System.out.println("Enter the temperature in Celsius");
  int celsius = keyboard.nextInt();
  int celsiusToFahrenheit = celsius *2 + 30;
  System.out.println("Degrees Fahrenheit is approx " 
    + celsiusToFahrenheit);
  keyboard.close();
  %@nonl
  %@-node:paran.20140530135904.1962:<<tempConv3>>
  %@nl
}
\end{lstlisting}

While this again expresses the same functionality as the code above it has not created a new method to do so. This has some of the same advantages. It separates and identifies the formula to convert between Celsius and Fahrenheit. It also uses less code to express this separation than forming a new method. It does not expose the conversion formula outside this method to be used by other calculations however.

As the value of a particular refactoring appears to depend on what is trying to be achieved it is very hard to claim that one refactoring is better than another. It depends entirely on the wider context of the intention for the refactoring, in this case the level of access required for the approximation to convert Celsius to Fahrenheit.

Although this was a simple example it is easy to imagine a case where a much larger refactoring process is undertaken. In such circumstances a merge becomes difficult. 
%@-node:paran.20140530135904.1958:<<Refactoring>>
%@nl

\section{JDime}
%@<<JDime>>
%@+node:paran.20140528183434.2021:<<JDime>>
Part of the inspiration for this tool come from JDime

\subsection{What Jdime can be used for}
%@<<What Jdime can be used for>>
%@+node:paran.20140528183434.2022:<<What Jdime can be used for>>
JDime can be used to compare two different versions of Java source codes which have been refactored and to produce a copy common to them both. 
%@nonl
%@-node:paran.20140528183434.2022:<<What Jdime can be used for>>
%@nl

\subsection{How Jdime works}
%@<<How Jdime works>>
%@+node:paran.20140528183434.2023:<<How Jdime works>>
JDime instead of testing against a source code repository test against files in the system under the base, left and right directories.
While this may be useful in quickly being able to show what JDime is able to achieve it requires that the inputs need to be previously extracted from a repository into the file system.

Before doing any calculations, JDime runs a regular text merge over the source code.  
If the regular text merge has conflicts then JDime parses the file into an abstract syntax tree (AST).  JDime uses the AST to determine if sections of the source code need to be in a particular order or could be in any order.
What then happens depends on if order is required in the section of code JDime is examining.
%@nonl
%@-node:paran.20140528183434.2023:<<How Jdime works>>
%@nl

\subsection{Testing Jdimes suitability}
%@<<Testing Jdimes suitability>>
%@+node:paran.20140528183434.2024:<<Testing Jdimes suitability>>
The work of Apel and Les{\ss}nich has mostly been done in Java.  There are a few exceptions including the linear programming libraries that need to be created.  As we are attempting to combine some of their work with Git it was decided to use JGit rather than the C implementation of Git. As the Java implementation may run a bit slower then in order to get a good timing test running we need to run redo the tests of JDime using JGit instead. 

Also the tests that Le{\ss}nich did on JDime were from files rather than from a repository. It is necessary to set the files back up in the original repository structure to get a adequate baseline.

As JDime performs a type of automatic merge it requires 3 different revisions.
JDime requires a revision that has changes that we want included.  This is commonly called the right revision however I will call this the merger revision as the changes in it are meant to be merged.
JDime also requires a revision that we want to merge into.  This is commonly called the left revision, however I will refer to this as being the mergee. 
Finally JDime requires an original revision that both the merger and the mergee are based on.
This is commonly called the base revision.

At the moment it cannot access a version control system so each of the revisions need to be set up as directories.
Each directory needs a full copy of the source code for that revision 
This means that the necessary Java source code to be used by JDime in base, left and right directories.


In order to test if JDime is better than a text based merge we need to attempt to try something that would incorrectly cause a conflict in a text based merge.
One way to get a lot of text conflicts between two pieces of code that are equivalent when they run is to change the order of the methods.
Although the methods are in different order the programs are still \"functionally equivalent\".
In order to examine how JDime works and test its suitability a test handler was written.
The test handler creates all of the directories and files for JDime to process.
The methods inside the files are reordered differently for both the left and the right directories.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=1]{JdimeTestSetup}
\end{center}
 \caption{The setup for the test of JDime}
\end{figure}

Once the test was set up using the test handler the JDime run to process the directories.
What we expected to happen was that JDime would reorder the methods to match the order in the mergee. When we compared the methods using a graphical merge tool however we found that the order of the methods in the files did not match.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.25]{DiffLeft}
\end{center}
 \caption{Screen-shot of Meld showing a different method order}
\end{figure}

It is about this point in analysis that you begin to second guess yourself.  For this reason the output of JDime was compared also with the merger and the base.  The order of the methods in the output did not match the order of any of the equivalent input files.  

If it detects an unordered section it does not preserve the order of the output.
%@nonl
%@-node:paran.20140528183434.2024:<<Testing Jdimes suitability>>
%@nl

\subsection{Conclusion}
%@<<Conclusion>>
%@+node:paran.20140604093616.2026:<<Conclusion>>
The aim of this paper is to be able to maintain two views of Java that although having a different format function in the same manner.  Although this tool sounds like it would be able to help achieve those aims there are a few reasons why it cannot be used without changes.

%@<<first issue>>
%@+node:paran.20140605091816.2391:<<first issue>>
The first issue is that as explained above that the resulting code could be in a totally different order to any of the versions combined to create it.
%@nonl
%@-node:paran.20140605091816.2391:<<first issue>>
%@nl

%@<<second issue>>
%@+node:paran.20140605091816.2392:<<second issue>>
The second issue is that when JDime parses the code into an AST it strips out any comments or white-space placed in the code.  Although the comments do not have any functional impact on how the program runs they do have an impact on how the source code is understood.  To limit the impact a merge makes on one view comments need to be evaluated as well. In some ways retaining comments or even white-space in the code aids in determining if a section of the code has been copied verbatim from one place to another.
%@nonl
%@-node:paran.20140605091816.2392:<<second issue>>
%@nl

%@<<third issue>>
%@+node:paran.20140605091816.2394:<<third issue>>
The only time an AST based merge is performed is if there are conflicts.  There could be advantage to determining if items that haven't conflicted in the text based merge but have moved from one position in the code to another.  If a method has both been moved and changed in both branches it could have a conflict.  This conflict would appear to the text-merge as a deletion agreed upon by both branches followed by two insertions at different points.  It would not be picked up by the text-merge as containing a conflict even if one was potentially present.  Since the conflict is not detected by a text-merge it is not set aside for testing by examining the AST tree. In this case comparing AST trees could detect that there was a conflict. Although JDime is an improvement over a text-based merge this is a potential conflict that niether detect. There is a performance reason for this design decision. JDime can take a long time to determine if two files contain equivalent source code.
%@nonl
%@-node:paran.20140605091816.2394:<<third issue>>
%@nl

%@<<final issue>>
%@+node:paran.20140605091816.2393:<<final issue>>
The final concern is that after JDime does the initial comparision of text and finds conflicts it discards those results. It parses the entire file into an AST and begins analysing it again without knowing which parts differ.   
%@nonl
%@-node:paran.20140605091816.2393:<<final issue>>
%@nl
%@-node:paran.20140604093616.2026:<<Conclusion>>
%@nl






%@-node:paran.20140528183434.2021:<<JDime>>
%@nl
 
 


%@-node:paran.20140515173914.6982:@shadow Background.tex
%@-leo
